{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_datos_climaticos(fecha_inicio, fecha_fin, api_key):\n",
    "    # URL de la API inicial\n",
    "    url = f\"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/{fecha_inicio}/fechafin/{fecha_fin}/todasestaciones\"\n",
    "    \n",
    "    # Encabezados de la solicitud\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'api_key': api_key\n",
    "    }\n",
    "\n",
    "    # Realizar la solicitud GET inicial\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Verificar el estado de la respuesta\n",
    "    if response.status_code == 200:\n",
    "        # Convertir la respuesta a JSON\n",
    "        data = response.json()\n",
    "        \n",
    "        # Obtener la URL de los datos reales\n",
    "        datos_url = data.get('datos')\n",
    "        \n",
    "        if datos_url:\n",
    "            # Realizar la segunda solicitud GET para obtener los datos reales\n",
    "            response_datos = requests.get(datos_url)\n",
    "            \n",
    "            if response_datos.status_code == 200:\n",
    "                # Extraer los datos en formato JSON\n",
    "                datos_reales = response_datos.json()\n",
    "                \n",
    "                # Verificar si datos_reales es una lista y convertirla a DataFrame\n",
    "                if isinstance(datos_reales, list):\n",
    "                    df = pd.DataFrame(datos_reales)\n",
    "                    \n",
    "                    # Filtrar el DataFrame para excluir las provincias no deseadas\n",
    "                    provincias_no_deseadas = ['ILLES BALEARS', 'STA. CRUZ DE TENERIFE', 'LAS PALMAS', 'CEUTA', 'MELILLA']\n",
    "                    df_filtrado = df[~df['provincia'].isin(provincias_no_deseadas)].copy()\n",
    "                    \n",
    "                    # Seleccionar solo los campos deseados\n",
    "                    df_final = df_filtrado[['fecha', 'tmed', 'hrMedia', 'provincia']].copy()\n",
    "                    \n",
    "                    # Reemplazar comas con puntos en los campos 'tmed' y 'hrMedia'\n",
    "                    df_final['tmed'] = df_final['tmed'].str.replace(',', '.', regex=False)\n",
    "                    df_final['hrMedia'] = df_final['hrMedia'].str.replace(',', '.', regex=False)\n",
    "                    \n",
    "                    # Convertir los campos 'tmed' y 'hrMedia' a tipo numérico\n",
    "                    df_final['tmed'] = pd.to_numeric(df_final['tmed'], errors='coerce')\n",
    "                    df_final['hrMedia'] = pd.to_numeric(df_final['hrMedia'], errors='coerce')\n",
    "                    \n",
    "                    # Agrupar por provincia y fecha y calcular la media de 'tmed' y 'hrMedia'\n",
    "                    df_provincia_fecha = df_final.groupby(['provincia', 'fecha']).agg({\n",
    "                        'tmed': 'mean',\n",
    "                        'hrMedia': 'mean'\n",
    "                    }).reset_index()\n",
    "                    \n",
    "                    # Agrupar por fecha para obtener la media total peninsular\n",
    "                    df_agrupado = df_provincia_fecha.groupby('fecha').agg({\n",
    "                        'tmed': 'mean',\n",
    "                        'hrMedia': 'mean'\n",
    "                    }).reset_index()\n",
    "                    \n",
    "                    return df_agrupado\n",
    "                else:\n",
    "                    print(\"La estructura de los datos no es una lista.\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"Error al obtener los datos reales: {response_datos.status_code}\")\n",
    "        else:\n",
    "            print(\"No se encontró la URL de los datos reales en la respuesta.\")\n",
    "    else:\n",
    "        print(f\"Error en la solicitud inicial: {response.status_code}, {response.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Asegúrate de importar el módulo time\n",
    "\n",
    "def datos_climaticos_anuales(year, api_key):\n",
    "    # Definir el rango de fechas para el año completo\n",
    "    fecha_inicio = f\"{year}-01-01T00:00:00UTC\"\n",
    "    fecha_final = f\"{year}-12-31T23:59:59UTC\"\n",
    "    \n",
    "    # Crear una lista para almacenar los DataFrames de cada intervalo\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterar por cada intervalo de 10 días\n",
    "    fecha_actual = datetime.strptime(f\"{year}-01-01\", '%Y-%m-%d')\n",
    "    fecha_final = datetime.strptime(f\"{year}-12-31\", '%Y-%m-%d')\n",
    "    \n",
    "    while fecha_actual <= fecha_final:\n",
    "        # Definir el rango de fechas para el intervalo actual\n",
    "        fecha_str_inicio = fecha_actual.strftime('%Y-%m-%dT00:00:00UTC')\n",
    "        fecha_str_fin = (fecha_actual + timedelta(days=9)).strftime('%Y-%m-%dT23:59:59UTC')\n",
    "        \n",
    "        # Asegurarse de no exceder la fecha final del año\n",
    "        if fecha_str_fin > f\"{year}-12-31T23:59:59UTC\":\n",
    "            fecha_str_fin = f\"{year}-12-31T23:59:59UTC\"\n",
    "        \n",
    "        # Obtener datos del intervalo actual\n",
    "        df_intervalo = obtener_datos_climaticos(fecha_str_inicio, fecha_str_fin, api_key)\n",
    "        \n",
    "        if df_intervalo is not None:\n",
    "            dfs.append(df_intervalo)\n",
    "        \n",
    "        # Pasar al siguiente intervalo de 10 días\n",
    "        fecha_actual += timedelta(days=10)\n",
    "        \n",
    "        # Retraso para evitar exceder el límite de peticiones\n",
    "        time.sleep(1)  # Esperar 1 segundo antes de la siguiente solicitud\n",
    "    \n",
    "    # Concatenar todos los DataFrames y calcular la media para cada fecha\n",
    "    df_anual = pd.concat(dfs).groupby('fecha').agg({\n",
    "        'tmed': 'mean',\n",
    "        'hrMedia': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return df_anual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_datos_climaticos_csv(dataframes, archivo='datos_climaticos_diarios.csv'):\n",
    "    # Concatenar todos los DataFrames y guardar en un archivo CSV\n",
    "    df_total = pd.concat(dataframes, ignore_index=True)\n",
    "    df_total.to_csv(archivo, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontró la URL de los datos reales en la respuesta.\n",
      "No se encontró la URL de los datos reales en la respuesta.\n",
      "No se encontró la URL de los datos reales en la respuesta.\n",
      "No se encontró la URL de los datos reales en la respuesta.\n",
      "No se encontró la URL de los datos reales en la respuesta.\n",
      "No se encontró la URL de los datos reales en la respuesta.\n",
      "No se encontró la URL de los datos reales en la respuesta.\n",
      "No se encontró la URL de los datos reales en la respuesta.\n",
      "No se encontró la URL de los datos reales en la respuesta.\n",
      "No se encontró la URL de los datos reales en la respuesta.\n"
     ]
    }
   ],
   "source": [
    "api_key = 'eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJncmVnb3Jpb3ZhbHZlcmRlOEBnbWFpbC5jb20iLCJqdGkiOiJkZGYyMDUzMy1lYmZmLTQ5MmUtOGYzNi05ZTE0YjEyNjUzZWUiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTcyNTk5NDA0NiwidXNlcklkIjoiZGRmMjA1MzMtZWJmZi00OTJlLThmMzYtOWUxNGIxMjY1M2VlIiwicm9sZSI6IiJ9.zgqzThSKPnGhAikI7O601zI9CTHsklcQ7Mk1UX83K-I'\n",
    "años = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "dataframes = []\n",
    "\n",
    "# Obtener datos para cada año y agregar al listado\n",
    "for year in años:\n",
    "    df_anual = datos_climaticos_anuales(year, api_key)\n",
    "    dataframes.append(df_anual)\n",
    "\n",
    "# Guardar todos los datos en un solo CSV\n",
    "guardar_datos_climaticos_csv(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
